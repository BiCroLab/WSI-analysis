{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '../py')\n",
    "from graviti import *\n",
    "\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "from os import path\n",
    "import sys\n",
    "import glob\n",
    "import h5py\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "import plotly.graph_objects as go\n",
    "from plotly.graph_objs import *\n",
    "import plotly.express as px\n",
    "import hdbscan\n",
    "import pandas as pd\n",
    "import umap\n",
    "import networkx as nx\n",
    "from scipy import sparse, linalg\n",
    "import pickle\n",
    "from sklearn.preprocessing import normalize, scale\n",
    "from scipy.sparse import find\n",
    "from numpy.linalg import norm\n",
    "import timeit\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 100000 # number of nuclei, use 0 value for full set\n",
    "nn = 10 # set the number of nearest neighbor in the umap-graph. Will be used in CovD as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in glob.glob('/media/garner1/hdd2/tcga.detection/*.gz'):\n",
    "for file in glob.glob('../data/tcga.detection/*.gz')[:1]:\n",
    "    \n",
    "    sample = os.path.basename(file).split(sep='.')[0]; print(sample)\n",
    "    \n",
    "    print('Loading the data')\n",
    "    df = pd.read_csv(file,sep='\\t').head(n=1000000)\n",
    "#    df = pd.read_csv(file,sep='\\t')\n",
    "\n",
    "    features = df.columns[7:]\n",
    "    centroids = df.columns[5:7]\n",
    "\n",
    "    print('Downsampling the data')\n",
    "    if size == 0:\n",
    "        fdf = df # filter out extremes in morphology\n",
    "    else:\n",
    "        fdf = df.sample(n=size) # filter out morphological outlyers and subsample nuclei\n",
    "    pos = fdf[centroids].to_numpy() # Get the positions of centroids \n",
    "    fdf = fdf.rename(columns={\"Centroid X µm\": \"cx\", \"Centroid Y µm\": \"cy\"})\n",
    "    \n",
    "    print('Creating the UMAP graph')\n",
    "    A = space2graph(pos,nn)\n",
    "    \n",
    "    print('Generation of the nn of the sampled nodes')\n",
    "    X = df[centroids].to_numpy() # the full array of position\n",
    "    n_neighbors = df.shape[0]//size + 10\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm='kd_tree',n_jobs=-1).fit(X) \n",
    "    distances, indices = nbrs.kneighbors(X) \n",
    "\n",
    "    data = scale(df[features].to_numpy(), with_mean=False) #get the morphological data and rescale the data by std \n",
    "    \n",
    "    # Get numb of cores\n",
    "    num_cores = multiprocessing.cpu_count() # numb of cores\n",
    "\n",
    "    # Parallel generation of the local covd\n",
    "    print('Generating the descriptor')\n",
    "    processed_list = Parallel(n_jobs=num_cores)(delayed(covd_parallel_sparse)(node,data,indices) \n",
    "                                                            for node in tqdm(list(fdf.index))\n",
    "                                                   )\n",
    "    \n",
    "    # Construct the descriptor array\n",
    "    descriptor = np.zeros((len(processed_list),processed_list[0][1].shape[0]))\n",
    "    for r in range(len(processed_list)):\n",
    "        descriptor[r,:] = processed_list[r][1] # covd descriptors of the connected nodes\n",
    "        \n",
    "    # Get info about the graph\n",
    "    row_idx, col_idx, values = find(A) #A.nonzero() # nonzero entries\n",
    "    print('Generating the diversity index')\n",
    "    node_nn_diversity = Parallel(n_jobs=num_cores)(delayed(covd_gradient_parallel)(node,descriptor,row_idx,col_idx,values) \n",
    "                               for node in tqdm(range(descriptor.shape[0])))\n",
    "    fdf['diversity'] = [sum(node_nn_diversity[node][2]) for node in range(descriptor.shape[0])]\n",
    "\n",
    "    filename = './'+str(sample)+'.size'+str(size)+'.graphNN'+str(nn)+'.covdNN'+str(n_neighbors)+'.tcga.pkl'\n",
    "    fdf.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_edges(fdf,node_nn_dist):\n",
    "edges = []\n",
    "for (node, neightbors, diversity) in node_nn_diversity:\n",
    "    node_arr = fdf.iloc[[node]][['cx','cy']].to_numpy()\n",
    "    nn_arr = fdf.iloc[neightbors][['cx','cy']].to_numpy()\n",
    "    centroid = 0.5*(node_arr+nn_arr)\n",
    "    array = np.hstack((centroid,diversity.reshape((diversity.shape[0],1))))\n",
    "    edges.extend(array.tolist())\n",
    "edge_df = pd.DataFrame(edges, columns=[\"cx\", \"cy\",\"diversity\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.scatter(edge_df[edge_df['diversity']<100], \n",
    "                 x=\"cx\", y=\"cy\", \n",
    "                 color=\"diversity\",\n",
    "                 #size = 'diversity', \n",
    "                 hover_data=['diversity'],\n",
    "                 width=800, height=800)\n",
    "fig.update_traces(marker=dict(size=1),selector=dict(mode='markers'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show contour plot\n",
    "N = 100\n",
    "filename = './'+str(sample)+'.size'+str(size)+'.graphNN'+str(nn)+'.covdNN'+str(n_neighbors)+'.bin'+str(N)+'.contour.tcga.sum.png'\n",
    "contourPlot(fdf[fdf[\"diversity\"]<100],N,np.mean,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show contour plot\n",
    "N = 200\n",
    "filename = './'+str(sample)+'.size'+str(size)+'.graphNN'+str(nn)+'.covdNN'+str(n_neighbors)+'.bin'+str(N)+'.contour.tcga.sum.png'\n",
    "contourPlot(edge_df[edge_df[\"diversity\"]<20],N,np.median,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "fdf['x_bin'] = pd.cut(fdf['cx'], 2*N, labels=False) # define the x bin label\n",
    "fdf['y_bin'] = pd.cut(fdf['cy'], N, labels=False) # define the y bin label\n",
    "\n",
    "table = pd.pivot_table(fdf,\n",
    "                       values='diversity',\n",
    "                       index=['x_bin'],\n",
    "                       columns=['y_bin'],\n",
    "                       aggfunc=np.sum, # take the mean of the entries in the bin\n",
    "                       fill_value=None\n",
    "                      )\n",
    "import numpy as np\n",
    "from skimage.io import imsave, imread\n",
    "\n",
    "image = np.array(np.nan_to_num(table.to_numpy()), dtype=np.uint8)\n",
    "\n",
    "imsave(\"test.png\", image)\n",
    "print(\"image shape\")\n",
    "print(image.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
