{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "from os import path\n",
    "import sys\n",
    "import glob\n",
    "import h5py\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import plotly.graph_objects as go\n",
    "from plotly.graph_objs import *\n",
    "import plotly.express as px\n",
    "import hdbscan\n",
    "import pandas as pd\n",
    "import umap\n",
    "from graviti import *\n",
    "import networkx as nx\n",
    "from scipy import sparse, linalg\n",
    "import pickle\n",
    "\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contour visualization\n",
    "def contourPlot(fdf,N,filename):\n",
    "    fdf['field'] = np.log(0.5*delta) # define the laplacian field and mult by 0.5 because of double counting edges\n",
    "    fdf['x_bin'] = pd.cut(fdf['Centroid X µm'], N, labels=False) # define the x bin label\n",
    "    fdf['y_bin'] = pd.cut(fdf['Centroid Y µm'], N, labels=False) # define the y bin label\n",
    "\n",
    "    # define the pivot tabel for the contour plot\n",
    "    table = pd.pivot_table(fdf, \n",
    "                           values='field', \n",
    "                           index=['x_bin'],\n",
    "                           columns=['y_bin'],\n",
    "                           aggfunc=np.sum, # take the mean of the entries in the bin\n",
    "                           fill_value=None)\n",
    "\n",
    "    X=table.columns.values\n",
    "    Y=table.index.values\n",
    "    Z=table.values\n",
    "    Xi,Yi = np.meshgrid(X, Y)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "    cs = ax.contourf(Yi, Xi, Z, \n",
    "                     alpha=1.0, \n",
    "                     levels=10,\n",
    "                     cmap=plt.cm.viridis) ;\n",
    "    cbar = fig.colorbar(cs)\n",
    "    plt.gca().invert_yaxis() # to vertically flip the image\n",
    "    plt.savefig(filename)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 0 # number of nuclei, use negative value for full set\n",
    "nn = 10 # set the number of nearest neighbor in the umap-graph. Will be used in CovD as well\n",
    "N = 100 # number of linear bins for the contour visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MN41\n",
      "Creating the graph\n",
      "Creating the network\n",
      "Generating the descriptor\n",
      "MN11\n",
      "Creating the graph\n",
      "Creating the network\n",
      "Generating the descriptor\n",
      "MN31\n",
      "Creating the graph\n",
      "Creating the network\n",
      "Generating the descriptor\n",
      "MN54\n",
      "Creating the graph\n",
      "Creating the network\n",
      "Generating the descriptor\n",
      "MN24\n",
      "Creating the graph\n",
      "Creating the network\n",
      "Generating the descriptor\n",
      "MN18\n",
      "Creating the graph\n",
      "Creating the network\n",
      "Generating the descriptor\n",
      "MN39\n",
      "Creating the graph\n",
      "Creating the network\n",
      "Generating the descriptor\n",
      "MN13\n",
      "Creating the graph\n",
      "Creating the network\n",
      "Generating the descriptor\n",
      "MN37\n",
      "Creating the graph\n",
      "Creating the network\n",
      "Generating the descriptor\n",
      "MN40\n",
      "Creating the graph\n",
      "Creating the network\n",
      "Generating the descriptor\n",
      "MN10\n",
      "Creating the graph\n",
      "Creating the network\n",
      "Generating the descriptor\n",
      "MN52\n",
      "Creating the graph\n",
      "Creating the network\n",
      "Generating the descriptor\n",
      "MN56\n",
      "Creating the graph\n",
      "Creating the network\n",
      "Generating the descriptor\n",
      "MN16\n",
      "Creating the graph\n",
      "Creating the network\n",
      "Generating the descriptor\n",
      "MN45\n",
      "Creating the graph\n",
      "Creating the network\n",
      "Generating the descriptor\n",
      "MN12\n",
      "Creating the graph\n",
      "Creating the network\n",
      "Generating the descriptor\n",
      "MN17\n",
      "Creating the graph\n",
      "Creating the network\n",
      "Generating the descriptor\n",
      "MN57\n",
      "Creating the graph\n",
      "Creating the network\n",
      "Generating the descriptor\n",
      "MN51\n",
      "Creating the graph\n",
      "Creating the network\n",
      "Generating the descriptor\n",
      "MN53\n",
      "Creating the graph\n",
      "Creating the network\n",
      "Generating the descriptor\n",
      "MN38\n",
      "Creating the graph\n",
      "Creating the network\n",
      "Generating the descriptor\n",
      "MN43\n",
      "Creating the graph\n",
      "Creating the network\n",
      "Generating the descriptor\n",
      "MN19\n",
      "Creating the graph\n",
      "Creating the network\n",
      "Generating the descriptor\n"
     ]
    }
   ],
   "source": [
    "for file in glob.glob('/home/garner1/Work/pipelines/WSI-analysis/SG/pipeline/data/HE/*.gz'):\n",
    "    \n",
    "    sample = os.path.basename(file).split(sep='.')[0]; print(sample)\n",
    "\n",
    "    df = pd.read_csv(file,sep='\\t')\n",
    "\n",
    "    features = df.columns[7:]\n",
    "    centroids = df.columns[5:7]\n",
    "\n",
    "    # filter by percentiles in morphologies (hardcoded in function filtering) and introduce coeff. of var\n",
    "    if size == 0:\n",
    "        fdf = filtering_HE(df) # filter out extremes in morphology\n",
    "    else:\n",
    "        fdf = filtering_HE(df).sample(n=size) # filter out morphological outlyers and subsample nuclei\n",
    "\n",
    "    pos = fdf[centroids].to_numpy() # Get the positions of centroids \n",
    "\n",
    "    # Building the UMAP graph\n",
    "    filename = '../py/ID'+str(sample)+'.size'+str(size)+'.nn'+str(nn)+'.graph.HE.npz' # the adj sparse matrix\n",
    "    if path.exists(filename):\n",
    "        print('The graph already exists')\n",
    "        A = sparse.load_npz(filename) \n",
    "    else:\n",
    "        print('Creating the graph')\n",
    "        A = space2graph(pos,nn)\n",
    "        sparse.save_npz(filename, A)\n",
    "\n",
    "\n",
    "    filename = '../py/ID'+str(sample)+'.size'+str(size)+'.nn'+str(nn)+'.graph.HE.pickle'    # the networkx obj\n",
    "    if path.exists(filename):    \n",
    "        print('The network already exists')\n",
    "        G = nx.read_gpickle(filename)\n",
    "    else:\n",
    "        print('Creating the network')\n",
    "        G = nx.from_scipy_sparse_matrix(A, edge_attribute='weight')\n",
    "        nx.write_gpickle(G, filename)\n",
    "\n",
    "    data = fdf[features].to_numpy() #get the morphological data\n",
    "\n",
    "    # Parallel generation of the local covd\n",
    "    filename = '../py/ID'+str(sample)+'.size'+str(size)+'.nn'+str(nn)+'.descriptor.HE.pickle'    # the descriptor\n",
    "    if path.exists(filename):    \n",
    "        print('The descriptor already exists')\n",
    "        descriptor = pickle.load( open( filename, \"rb\" ) )\n",
    "    else:\n",
    "        print('Generating the descriptor')\n",
    "        num_cores = multiprocessing.cpu_count() # numb of cores\n",
    "        row_idx, col_idx = A.nonzero() # nonzero entries\n",
    "        processed_list = Parallel(n_jobs=num_cores)(delayed(covd_local)(r,A,data,row_idx,col_idx) \n",
    "                                                                for r in range(A.shape[0])\n",
    "                                                       )\n",
    "\n",
    "        # Construct the descriptor array\n",
    "        descriptor = np.zeros((len(processed_list),processed_list[0][1].shape[0]))\n",
    "        for r in range(len(processed_list)):\n",
    "            descriptor[r,:] = processed_list[r][1] # covd descriptors of the connected nodes\n",
    "        pickle.dump( descriptor, open( filename, \"wb\" ) )\n",
    "\n",
    "    # Construct the local Laplacian\n",
    "    L = nx.laplacian_matrix(G, weight='weight') # get the Laplacian matrix\n",
    "    delta_descriptor = L.dot(descriptor) # get the local differianted descriptor\n",
    "    delta = norm(delta_descriptor,axis=1) # get the norm of the differential field\n",
    "    filename = './ID.'+str(sample)+'.size'+str(size)+'.nn'+str(nn)+'.bin'+str(N)+'.contour.HE.png'\n",
    "    contourPlot(fdf,N,filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
